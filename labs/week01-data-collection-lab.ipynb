{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Lab: Data Collection for Machine Learning\n",
    "\n",
    "**CS 203: Software Tools and Techniques for AI**\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "In this lab, you will learn to collect data from the web using:\n",
    "1. **HTTP fundamentals** - Understanding how the web works\n",
    "2. **curl** - Command-line HTTP client\n",
    "3. **Python requests** - Programmatic API calls\n",
    "4. **BeautifulSoup** - Web scraping when APIs don't exist\n",
    "\n",
    "**Goal**: Build a movie data collection pipeline for Netflix-style movie prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install and import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: HTTP Fundamentals\n",
    "\n",
    "Before we start collecting data, we need to understand how the web works.\n",
    "\n",
    "## 1.1 Understanding URLs\n",
    "\n",
    "A URL (Uniform Resource Locator) has several components:\n",
    "\n",
    "```\n",
    "https://api.omdbapi.com:443/v1/movies?t=Inception&y=2010#details\n",
    "└─┬──┘ └──────┬───────┘└┬─┘└───┬───┘└─────────┬────────┘└───┬───┘\n",
    "  │           │         │      │              │             │\n",
    "Protocol    Host      Port   Path          Query        Fragment\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1 (Solved): Parse a URL\n",
    "\n",
    "Use Python's `urllib.parse` to break down a URL into its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "url = \"https://api.omdbapi.com/?apikey=demo&t=Inception&y=2010\"\n",
    "\n",
    "parsed = urlparse(url)\n",
    "\n",
    "print(f\"Scheme (protocol): {parsed.scheme}\")\n",
    "print(f\"Host (domain): {parsed.netloc}\")\n",
    "print(f\"Path: {parsed.path}\")\n",
    "print(f\"Query string: {parsed.query}\")\n",
    "\n",
    "# Parse query parameters into a dictionary\n",
    "params = parse_qs(parsed.query)\n",
    "print(f\"\\nParsed parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: Parse a Different URL\n",
    "\n",
    "Parse the following GitHub API URL and extract:\n",
    "1. The host\n",
    "2. The path\n",
    "3. All query parameters as a dictionary\n",
    "\n",
    "URL: `https://api.github.com/search/repositories?q=machine+learning&sort=stars&order=desc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "url = \"https://api.github.com/search/repositories?q=machine+learning&sort=stars&order=desc\"\n",
    "\n",
    "# Parse the URL\n",
    "\n",
    "# Print the host\n",
    "\n",
    "# Print the path\n",
    "\n",
    "# Print the query parameters as a dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 HTTP Status Codes\n",
    "\n",
    "HTTP status codes tell you what happened with your request:\n",
    "\n",
    "| Range | Category | Common Examples |\n",
    "|-------|----------|----------------|\n",
    "| 2xx | Success | 200 OK, 201 Created |\n",
    "| 3xx | Redirect | 301 Moved, 302 Found |\n",
    "| 4xx | Client Error | 400 Bad Request, 401 Unauthorized, 404 Not Found |\n",
    "| 5xx | Server Error | 500 Internal Error, 503 Service Unavailable |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3: Match Status Codes\n",
    "\n",
    "Match each scenario to the most likely HTTP status code:\n",
    "\n",
    "1. You requested a movie that doesn't exist in the database\n",
    "2. You made too many requests and hit the rate limit\n",
    "3. Your API key is invalid\n",
    "4. The request was successful and data was returned\n",
    "5. The server crashed while processing your request\n",
    "\n",
    "Status codes to choose from: `200`, `401`, `404`, `429`, `500`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWERS HERE\n",
    "answers = {\n",
    "    \"movie_not_found\": None,      # Replace None with the status code\n",
    "    \"rate_limited\": None,\n",
    "    \"invalid_api_key\": None,\n",
    "    \"success\": None,\n",
    "    \"server_crashed\": None\n",
    "}\n",
    "\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Making Requests with `curl`\n",
    "\n",
    "`curl` is a command-line tool for making HTTP requests. It's essential for quick testing.\n",
    "\n",
    "## 2.1 Basic curl Commands\n",
    "\n",
    "You can run shell commands in Jupyter using `!` prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1 (Solved): Your First API Call\n",
    "\n",
    "Let's call a simple public API that requires no authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "# JSONPlaceholder is a free fake API for testing\n",
    "!curl -s \"https://jsonplaceholder.typicode.com/posts/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: Pretty Print with jq\n",
    "\n",
    "The output above is hard to read. Use `jq` to format it nicely.\n",
    "\n",
    "**Hint**: Pipe the curl output to jq: `curl ... | jq .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Fetch the same post but format the output with jq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3: Extract Specific Fields with jq\n",
    "\n",
    "Fetch all posts from `https://jsonplaceholder.typicode.com/posts` and extract only the `title` field from each post.\n",
    "\n",
    "**Hint**: Use `jq '.[].title'` to get the title from each element in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4: View Response Headers\n",
    "\n",
    "Use the `-I` flag to fetch only the response headers (no body) from:\n",
    "`https://api.github.com`\n",
    "\n",
    "What is the value of the `X-RateLimit-Limit` header?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.5: Add Custom Headers\n",
    "\n",
    "Make a request to `https://httpbin.org/headers` with the following custom headers:\n",
    "- `User-Agent: CS203-Lab/1.0`\n",
    "- `Accept: application/json`\n",
    "\n",
    "**Hint**: Use `-H \"Header-Name: value\"` for each header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Python `requests` Library\n",
    "\n",
    "While `curl` is great for testing, we need Python for automation.\n",
    "\n",
    "## 3.1 Basic GET Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 (Solved): Simple GET Request\n",
    "\n",
    "Make a GET request and inspect the response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"https://jsonplaceholder.typicode.com/posts/1\")\n",
    "\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(f\"Content-Type: {response.headers['Content-Type']}\")\n",
    "print(f\"Response OK: {response.ok}\")\n",
    "print(f\"\\nJSON Data:\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2: Fetch Multiple Posts\n",
    "\n",
    "Fetch posts from `https://jsonplaceholder.typicode.com/posts` and:\n",
    "1. Print the total number of posts\n",
    "2. Print the titles of the first 5 posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3 (Solved): Using Query Parameters\n",
    "\n",
    "The proper way to add query parameters is using the `params` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "import requests\n",
    "\n",
    "# Bad way (manual string building)\n",
    "# url = \"https://jsonplaceholder.typicode.com/posts?userId=1\"\n",
    "\n",
    "# Good way (using params)\n",
    "response = requests.get(\n",
    "    \"https://jsonplaceholder.typicode.com/posts\",\n",
    "    params={\"userId\": 1}\n",
    ")\n",
    "\n",
    "posts = response.json()\n",
    "print(f\"User 1 has {len(posts)} posts\")\n",
    "print(f\"\\nActual URL used: {response.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.4: Filter Posts by User\n",
    "\n",
    "Fetch all posts by user 5 and user 7. Compare how many posts each user has.\n",
    "\n",
    "**Hint**: Make two separate requests with different `userId` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.2 Working with Real APIs\n",
    "\n",
    "Let's work with some real-world APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.5 (Solved): GitHub API - Public Repositories\n",
    "\n",
    "The GitHub API is free to use (with rate limits) and doesn't require authentication for public data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "import requests\n",
    "\n",
    "# Fetch information about a popular repository\n",
    "response = requests.get(\n",
    "    \"https://api.github.com/repos/pandas-dev/pandas\",\n",
    "    headers={\"Accept\": \"application/vnd.github.v3+json\"}\n",
    ")\n",
    "\n",
    "if response.ok:\n",
    "    repo = response.json()\n",
    "    print(f\"Repository: {repo['full_name']}\")\n",
    "    print(f\"Description: {repo['description']}\")\n",
    "    print(f\"Stars: {repo['stargazers_count']:,}\")\n",
    "    print(f\"Forks: {repo['forks_count']:,}\")\n",
    "    print(f\"Language: {repo['language']}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.6: Compare Popular ML Libraries\n",
    "\n",
    "Fetch information about these ML-related repositories and create a comparison table:\n",
    "- `scikit-learn/scikit-learn`\n",
    "- `pytorch/pytorch`\n",
    "- `tensorflow/tensorflow`\n",
    "\n",
    "Show: name, stars, forks, and primary language.\n",
    "\n",
    "**Hint**: Loop through the repos and collect data into a list of dictionaries, then create a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "repos = [\n",
    "    \"scikit-learn/scikit-learn\",\n",
    "    \"pytorch/pytorch\",\n",
    "    \"tensorflow/tensorflow\"\n",
    "]\n",
    "\n",
    "# Fetch data for each repo\n",
    "\n",
    "# Create a DataFrame\n",
    "\n",
    "# Display the comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.7: Search GitHub Repositories\n",
    "\n",
    "Use the GitHub search API to find the top 10 most starred repositories with \"machine learning\" in their description.\n",
    "\n",
    "API endpoint: `https://api.github.com/search/repositories`\n",
    "\n",
    "Parameters:\n",
    "- `q`: search query (e.g., \"machine learning\")\n",
    "- `sort`: \"stars\"\n",
    "- `order`: \"desc\"\n",
    "- `per_page`: 10\n",
    "\n",
    "Print the name and star count of each repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.3 Error Handling\n",
    "\n",
    "Real-world APIs fail. We need to handle errors gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.8 (Solved): Handling HTTP Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "import requests\n",
    "\n",
    "def fetch_with_error_handling(url):\n",
    "    \"\"\"Fetch URL with proper error handling.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raises exception for 4xx/5xx\n",
    "        return response.json()\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout: Request took too long\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error: {e.response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "    return None\n",
    "\n",
    "# Test with valid URL\n",
    "print(\"Valid URL:\")\n",
    "data = fetch_with_error_handling(\"https://jsonplaceholder.typicode.com/posts/1\")\n",
    "if data:\n",
    "    print(f\"  Got post: {data['title'][:50]}...\")\n",
    "\n",
    "# Test with invalid URL (404)\n",
    "print(\"\\nInvalid URL (404):\")\n",
    "fetch_with_error_handling(\"https://jsonplaceholder.typicode.com/posts/99999\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.9: Robust Fetcher Function\n",
    "\n",
    "Write a function `safe_fetch(url, max_retries=3)` that:\n",
    "1. Attempts to fetch the URL\n",
    "2. If it fails with a 5xx error, retries up to `max_retries` times\n",
    "3. Waits 1 second between retries\n",
    "4. Returns the JSON data if successful, None otherwise\n",
    "\n",
    "Test it with `https://httpbin.org/status/500` (always returns 500) and `https://jsonplaceholder.typicode.com/posts/1` (always works)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import time\n",
    "\n",
    "def safe_fetch(url, max_retries=3):\n",
    "    \"\"\"Fetch URL with retry logic for server errors.\"\"\"\n",
    "    pass  # Implement this\n",
    "\n",
    "# Test your function\n",
    "# print(\"Testing with working URL:\")\n",
    "# result = safe_fetch(\"https://jsonplaceholder.typicode.com/posts/1\")\n",
    "# print(f\"Result: {result}\")\n",
    "\n",
    "# print(\"\\nTesting with failing URL (500):\")\n",
    "# result = safe_fetch(\"https://httpbin.org/status/500\")\n",
    "# print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: The OMDb Movie API\n",
    "\n",
    "Now let's work with the OMDb API - our main data source for the Netflix project.\n",
    "\n",
    "**Note**: You need an API key from https://www.omdbapi.com/apikey.aspx (free tier available).\n",
    "\n",
    "For this lab, we'll use a demo key that has limited functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key here\n",
    "# Get a free key from: https://www.omdbapi.com/apikey.aspx\n",
    "OMDB_API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your actual key\n",
    "\n",
    "# For demo purposes, you can try with key \"demo\" but it's very limited\n",
    "# OMDB_API_KEY = \"demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1 (Solved): Fetch a Single Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "import requests\n",
    "\n",
    "def fetch_movie(title, year=None, api_key=OMDB_API_KEY):\n",
    "    \"\"\"Fetch movie data from OMDb API.\"\"\"\n",
    "    params = {\n",
    "        \"apikey\": api_key,\n",
    "        \"t\": title,  # Search by title\n",
    "        \"type\": \"movie\"\n",
    "    }\n",
    "    if year:\n",
    "        params[\"y\"] = year\n",
    "    \n",
    "    response = requests.get(\"https://www.omdbapi.com/\", params=params)\n",
    "    \n",
    "    if response.ok:\n",
    "        data = response.json()\n",
    "        if data.get(\"Response\") == \"True\":\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"Movie not found: {data.get('Error')}\")\n",
    "    return None\n",
    "\n",
    "# Fetch Inception\n",
    "movie = fetch_movie(\"Inception\", 2010)\n",
    "if movie:\n",
    "    print(f\"Title: {movie['Title']}\")\n",
    "    print(f\"Year: {movie['Year']}\")\n",
    "    print(f\"Director: {movie['Director']}\")\n",
    "    print(f\"IMDB Rating: {movie['imdbRating']}\")\n",
    "    print(f\"Genre: {movie['Genre']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2: Explore the Response\n",
    "\n",
    "Fetch data for \"The Dark Knight\" and print ALL available fields in the response.\n",
    "\n",
    "Which fields might be useful for predicting movie success?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3: Fetch Multiple Movies\n",
    "\n",
    "Create a function `fetch_movies(titles)` that:\n",
    "1. Takes a list of movie titles\n",
    "2. Fetches data for each movie\n",
    "3. Returns a list of movie dictionaries (only successful fetches)\n",
    "4. Adds a 0.5 second delay between requests (to respect rate limits)\n",
    "\n",
    "Test it with: `[\"Inception\", \"The Matrix\", \"Interstellar\", \"NonExistentMovie123\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def fetch_movies(titles):\n",
    "    \"\"\"Fetch multiple movies from OMDb API.\"\"\"\n",
    "    pass  # Implement this\n",
    "\n",
    "# Test\n",
    "# test_titles = [\"Inception\", \"The Matrix\", \"Interstellar\", \"NonExistentMovie123\"]\n",
    "# movies = fetch_movies(test_titles)\n",
    "# print(f\"Successfully fetched {len(movies)} out of {len(test_titles)} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.4: Create a Movie DataFrame\n",
    "\n",
    "Using the movies you fetched, create a pandas DataFrame with these columns:\n",
    "- title\n",
    "- year (as integer)\n",
    "- genre\n",
    "- director\n",
    "- imdb_rating (as float)\n",
    "- imdb_votes (as integer, remove commas)\n",
    "- runtime_minutes (as integer, extract from \"148 min\")\n",
    "- box_office (keep as string for now)\n",
    "\n",
    "**Hint**: You'll need to clean the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.5: Search Movies by Title\n",
    "\n",
    "OMDb also has a search endpoint that returns multiple results.\n",
    "\n",
    "Use the `s` parameter instead of `t` to search for movies containing \"Star Wars\".\n",
    "\n",
    "API endpoint: `https://www.omdbapi.com/?apikey=YOUR_KEY&s=Star Wars&type=movie`\n",
    "\n",
    "Print the title and year of each result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.6: Handle Pagination\n",
    "\n",
    "The OMDb search API returns 10 results per page and includes a `totalResults` field.\n",
    "\n",
    "Write a function `search_all_movies(query)` that:\n",
    "1. Searches for movies matching the query\n",
    "2. Fetches ALL pages of results (use the `page` parameter)\n",
    "3. Returns a list of all movies found\n",
    "\n",
    "**Hint**: `totalResults` tells you how many movies exist. Divide by 10 to get the number of pages.\n",
    "\n",
    "Test with a query that has many results like \"Batman\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def search_all_movies(query, api_key=OMDB_API_KEY):\n",
    "    \"\"\"Search OMDb and return ALL matching movies across all pages.\"\"\"\n",
    "    pass  # Implement this\n",
    "\n",
    "# Test\n",
    "# all_batman = search_all_movies(\"Batman\")\n",
    "# print(f\"Found {len(all_batman)} Batman movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Web Scraping with BeautifulSoup\n",
    "\n",
    "When APIs don't exist or don't have what we need, we scrape.\n",
    "\n",
    "## 5.1 HTML Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1 (Solved): Parse HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <div class=\"movie\" id=\"movie-1\">\n",
    "        <h2 class=\"title\">Inception</h2>\n",
    "        <span class=\"year\">2010</span>\n",
    "        <span class=\"rating\">8.8</span>\n",
    "        <a href=\"/movies/inception\">More Info</a>\n",
    "    </div>\n",
    "    <div class=\"movie\" id=\"movie-2\">\n",
    "        <h2 class=\"title\">The Matrix</h2>\n",
    "        <span class=\"year\">1999</span>\n",
    "        <span class=\"rating\">8.7</span>\n",
    "        <a href=\"/movies/matrix\">More Info</a>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Find all movie divs\n",
    "movies = soup.find_all('div', class_='movie')\n",
    "print(f\"Found {len(movies)} movies\\n\")\n",
    "\n",
    "# Extract data from each\n",
    "for movie in movies:\n",
    "    title = movie.find('h2', class_='title').text\n",
    "    year = movie.find('span', class_='year').text\n",
    "    rating = movie.find('span', class_='rating').text\n",
    "    link = movie.find('a')['href']\n",
    "    \n",
    "    print(f\"{title} ({year}) - Rating: {rating} - Link: {link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2: CSS Selectors\n",
    "\n",
    "Rewrite the above extraction using CSS selectors (`.select()` and `.select_one()`) instead of `.find()` and `.find_all()`.\n",
    "\n",
    "**Hint**: \n",
    "- `.movie` selects elements with class \"movie\"\n",
    "- `.movie .title` selects elements with class \"title\" inside class \"movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Use the same 'soup' from above\n",
    "\n",
    "# Extract using CSS selectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.3: Scrape a Real Website\n",
    "\n",
    "Let's scrape the example website `http://quotes.toscrape.com/` which is designed for scraping practice.\n",
    "\n",
    "Extract all quotes from the first page, including:\n",
    "- The quote text\n",
    "- The author name\n",
    "- The tags\n",
    "\n",
    "Return the results as a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the page\n",
    "url = \"http://quotes.toscrape.com/\"\n",
    "\n",
    "# Parse the HTML\n",
    "\n",
    "# Extract quotes\n",
    "\n",
    "# Print results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.4: Handle Pagination in Scraping\n",
    "\n",
    "The quotes website has multiple pages. Scrape the first 3 pages and collect all quotes.\n",
    "\n",
    "Pages follow the pattern:\n",
    "- Page 1: `http://quotes.toscrape.com/page/1/`\n",
    "- Page 2: `http://quotes.toscrape.com/page/2/`\n",
    "- etc.\n",
    "\n",
    "**Remember**: Add a delay between requests to be polite!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.5: Extract Table Data\n",
    "\n",
    "Scrape the table from `https://www.w3schools.com/html/html_tables.asp`.\n",
    "\n",
    "The table contains company data. Extract all rows and create a pandas DataFrame.\n",
    "\n",
    "**Hint**: Look for `<table>`, `<tr>` (table row), `<th>` (header), and `<td>` (data cell) elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: pandas has a read_html() function that can do this automatically!\n",
    "# But try doing it manually first to understand the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Building the Movie Data Pipeline\n",
    "\n",
    "Now let's put everything together to build a complete data collection pipeline for our Netflix project.\n",
    "\n",
    "## 6.1 The Complete Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.1 (Solved): Movie Data Collector Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class MovieDataCollector:\n",
    "    \"\"\"Collect movie data from OMDb API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://www.omdbapi.com/\"\n",
    "        self.delay = 0.5  # Seconds between requests\n",
    "    \n",
    "    def fetch_movie(self, title: str, year: Optional[int] = None) -> Optional[Dict]:\n",
    "        \"\"\"Fetch a single movie by title.\"\"\"\n",
    "        params = {\n",
    "            \"apikey\": self.api_key,\n",
    "            \"t\": title,\n",
    "            \"type\": \"movie\"\n",
    "        }\n",
    "        if year:\n",
    "            params[\"y\"] = year\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.base_url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if data.get(\"Response\") == \"True\":\n",
    "                return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {title}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fetch_movies(self, titles: List[str]) -> List[Dict]:\n",
    "        \"\"\"Fetch multiple movies.\"\"\"\n",
    "        movies = []\n",
    "        \n",
    "        for i, title in enumerate(titles):\n",
    "            print(f\"Fetching {i+1}/{len(titles)}: {title}\")\n",
    "            movie = self.fetch_movie(title)\n",
    "            \n",
    "            if movie:\n",
    "                movies.append(movie)\n",
    "            \n",
    "            time.sleep(self.delay)\n",
    "        \n",
    "        return movies\n",
    "    \n",
    "    def to_dataframe(self, movies: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Convert movie data to cleaned DataFrame.\"\"\"\n",
    "        if not movies:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Extract relevant fields\n",
    "        rows = []\n",
    "        for m in movies:\n",
    "            rows.append({\n",
    "                \"title\": m.get(\"Title\"),\n",
    "                \"year\": m.get(\"Year\"),\n",
    "                \"genre\": m.get(\"Genre\"),\n",
    "                \"director\": m.get(\"Director\"),\n",
    "                \"actors\": m.get(\"Actors\"),\n",
    "                \"imdb_rating\": m.get(\"imdbRating\"),\n",
    "                \"imdb_votes\": m.get(\"imdbVotes\"),\n",
    "                \"runtime\": m.get(\"Runtime\"),\n",
    "                \"box_office\": m.get(\"BoxOffice\"),\n",
    "                \"imdb_id\": m.get(\"imdbID\")\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        # Clean data types\n",
    "        df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df[\"imdb_rating\"] = pd.to_numeric(df[\"imdb_rating\"], errors=\"coerce\")\n",
    "        df[\"imdb_votes\"] = df[\"imdb_votes\"].str.replace(\",\", \"\").pipe(pd.to_numeric, errors=\"coerce\").astype(\"Int64\")\n",
    "        df[\"runtime_min\"] = df[\"runtime\"].str.extract(r\"(\\d+)\").pipe(pd.to_numeric, errors=\"coerce\").astype(\"Int64\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Usage example\n",
    "# collector = MovieDataCollector(OMDB_API_KEY)\n",
    "# movies = collector.fetch_movies([\"Inception\", \"The Matrix\"])\n",
    "# df = collector.to_dataframe(movies)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.2: Add Search Functionality\n",
    "\n",
    "Extend the `MovieDataCollector` class to add a `search_movies(query, max_results=50)` method that:\n",
    "1. Searches for movies matching the query\n",
    "2. Handles pagination to get up to `max_results` movies\n",
    "3. For each search result, fetches the full movie details\n",
    "4. Returns the detailed movie data\n",
    "\n",
    "**Hint**: Search results only contain basic info (title, year, poster, imdbID). You need to use the imdbID to fetch full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Extend the MovieDataCollector class or add a method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.3: Build a Genre-Based Dataset\n",
    "\n",
    "Use your collector to build a dataset of popular movies from different genres:\n",
    "\n",
    "1. Search for 10 movies each for: \"action\", \"comedy\", \"drama\", \"horror\", \"sci-fi\"\n",
    "2. Combine all results into a single DataFrame\n",
    "3. Remove any duplicates (some movies might appear in multiple searches)\n",
    "4. Save to CSV\n",
    "\n",
    "**Note**: This might take a while due to rate limiting. Start with fewer movies for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.4: Data Quality Analysis\n",
    "\n",
    "Using the dataset you created:\n",
    "\n",
    "1. How many movies have missing IMDB ratings?\n",
    "2. How many movies have missing box office data?\n",
    "3. What's the distribution of ratings? (min, max, mean, median)\n",
    "4. Which directors appear most frequently?\n",
    "5. What's the average runtime by genre?\n",
    "\n",
    "These quality checks will be important for Week 2 (Data Validation)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Challenge Problems\n",
    "\n",
    "These are optional advanced exercises for those who finish early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 7.1: Rate Limit Handler\n",
    "\n",
    "Create a `RateLimiter` class that:\n",
    "1. Tracks how many requests have been made\n",
    "2. Automatically adds delays to stay under a rate limit\n",
    "3. Handles 429 (Too Many Requests) responses by waiting and retrying\n",
    "\n",
    "```python\n",
    "limiter = RateLimiter(requests_per_minute=30)\n",
    "response = limiter.get(\"https://api.example.com/data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 7.2: Async Movie Collector\n",
    "\n",
    "The synchronous approach is slow because we wait for each request to complete.\n",
    "\n",
    "Create an async version using `aiohttp` that can fetch multiple movies concurrently (while still respecting rate limits).\n",
    "\n",
    "Compare the time to fetch 20 movies with sync vs async approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: You'll need to install aiohttp: pip install aiohttp\n",
    "# And use asyncio to run the async code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 7.3: Multi-Source Data Fusion\n",
    "\n",
    "Create a data collection pipeline that:\n",
    "1. Fetches basic movie data from OMDb\n",
    "2. Enriches it with additional data from another source (e.g., Wikipedia API for plot summaries)\n",
    "3. Merges the data based on movie title/year\n",
    "4. Handles cases where data is missing from one source\n",
    "\n",
    "Wikipedia API example:\n",
    "```\n",
    "https://en.wikipedia.org/api/rest_v1/page/summary/Inception_(film)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **HTTP Fundamentals**: URLs, status codes, headers\n",
    "2. **curl**: Command-line HTTP requests\n",
    "3. **Python requests**: Programmatic data collection\n",
    "4. **Error handling**: Timeouts, retries, status codes\n",
    "5. **OMDb API**: Real-world movie data\n",
    "6. **BeautifulSoup**: Web scraping when APIs don't exist\n",
    "7. **Data pipelines**: Building reusable collection code\n",
    "\n",
    "## Next Week\n",
    "\n",
    "**Week 2: Data Validation & Quality**\n",
    "\n",
    "The data we collected today is messy! Next week we'll learn:\n",
    "- Schema validation with Pydantic\n",
    "- Data type cleaning\n",
    "- Handling missing values\n",
    "- Quality metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Submission\n",
    "\n",
    "Save your completed notebook and submit:\n",
    "1. This notebook with all cells executed\n",
    "2. The CSV file of movies you collected\n",
    "3. A brief summary (1 paragraph) of what you learned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
